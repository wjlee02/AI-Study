{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":80316,"databundleVersionId":8589663,"sourceType":"competition"},{"sourceId":8532745,"sourceType":"datasetVersion","datasetId":5096355},{"sourceId":8583611,"sourceType":"datasetVersion","datasetId":5133582}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Practice 4 - Question & Answering with BERT","metadata":{}},{"cell_type":"markdown","source":"### 실습 4.1 - Load SQuAD Raw Data with JSON\n### SQuAD 데이터 살펴보기","metadata":{}},{"cell_type":"code","source":"import os\nimport json\n\nfilename = \"/kaggle/input/2024-1-nlp-4/train-v2.json\"\n\nwith open(filename, \"r\", encoding='utf-8') as reader:\n    input_data = json.load(reader)[\"data\"]\n    \nfor entry in input_data:\n    for paragraph in entry[\"paragraphs\"]:\n        context = paragraph['context']\n        print(context)\n        print()\n        \n        for qa in paragraph['qas']:\n            is_impossible = qa['is_impossible']\n\n            if not is_impossible:\n                answer = qa['answers'][0]\n                original_answer = answer['text']\n                answer_start = answer['answer_start']\n                \n            qid=qa['id'],\n            question=qa['question'],\n                \n            print(qid, question, answer)\n        \n    \n        break\n    break","metadata":{"execution":{"iopub.status.busy":"2024-05-27T15:48:05.493316Z","iopub.execute_input":"2024-05-27T15:48:05.494118Z","iopub.status.idle":"2024-05-27T15:48:07.035080Z","shell.execute_reply.started":"2024-05-27T15:48:05.494088Z","shell.execute_reply":"2024-05-27T15:48:07.034126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 실습 4.2 - SQuAD Dataset Class 생성 (from raw data to tokenized version)","metadata":{}},{"cell_type":"markdown","source":"주석 달아라","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom torch.utils.data import Dataset, TensorDataset\n\n# import module we'll need to import our custom module\nfrom shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"/kaggle/input/2024-1-nlp-4/feature.py\", dst = \"/kaggle/working/feature.py\")\nfrom feature import convert_examples_to_features\n\ndef is_whitespace(c):\n    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n        return True\n    return False\n\nclass SquadExample():\n    def __init__(self, qid, context, question, answer, start, end, is_impossible):\n        self.qid = qid\n        self.context = context\n        self.question = question\n        self.answer = answer\n        self.start = start\n        self.end = end\n        self.is_impossible = is_impossible\n        \n    def __repr__(self):\n        #return self.context[self.start:self.end]\n        #if self.context[self.start:self.end] != self.answer:\n        #    return 'NA!! {} - {}'.format(self.context[self.start:self.end], answer)\n        return 'id:{}  question:{}...  answer:{}...  is_impossible:{}'.format(\n            self.qid,\n            self.question[:10],\n            self.answer[:10],\n            self.is_impossible)\n\nclass SquadDataset(Dataset):\n    def __init__(self, path, tokenizer, is_train=True, is_inference=False):\n        '''\n        path: SquadDataset 데이터셋 위치\n        tokenizer: Squad 데이터셋을 토크나이징할 토크나이저, ex) BertTokenizer\n        is_train: SquadDataset을 정의하는 목적이 모델 학습용일 경우 True, 그렇지 않으면 False\n        is_inference: SquadDataset을 정의하는 목적이 인퍼런스용일 경우 True, 그렇지 않으면 False\n        '''\n        \n        if is_train:\n            filename = os.path.join(path, 'train-v2.json')\n        else:\n            if is_inference:\n                filename = os.path.join(path, 'test-v2.json')\n            else:\n                filename = os.path.join(path, 'dev-v2.json')\n\n        cached_features_file = os.path.join(os.path.dirname(filename), 'cached_{}_64.cache'.format('train' if is_train else 'valid'))\n        #cached_examples_file = os.path.join(os.path.dirname(filename), 'cached_example_{}_64.cache'.format('train' if is_train else 'valid'))\n\n        if os.path.exists(cached_features_file):\n            print('cache file exists')\n            self.features = torch.load(cached_features_file)\n        else:\n            print('cache file does not exist')\n\n            with open(filename, \"r\", encoding='utf-8') as reader:\n                input_data = json.load(reader)[\"data\"]\n\n            self.examples = []\n#             n_o_e = 100\n#             for entry in input_data[:n_o_e]:\n            for entry in input_data:\n                for paragraph in entry[\"paragraphs\"]:\n                    context = paragraph['context']\n                    \n                    doc_tokens = []\n                    char_to_word_offset = []\n                    prev_is_whitespace = True\n                    for c in context:\n                        if is_whitespace(c):\n                            prev_is_whitespace = True\n                        else:\n                            if prev_is_whitespace:\n                                doc_tokens.append(c)\n                            else:\n                                doc_tokens[-1] += c\n                            prev_is_whitespace = False\n                        char_to_word_offset.append(len(doc_tokens) - 1)\n                            \n                            \n                    for qa in paragraph['qas']:\n                        is_impossible = qa['is_impossible']\n                        \n                        if not is_impossible:\n                            answer = qa['answers'][0]\n                            original_answer = answer['text']\n                            answer_start = answer['answer_start']\n                            \n                            answer_length = len(original_answer)\n                            start_pos = char_to_word_offset[answer_start]\n                            end_pos = char_to_word_offset[answer_start + answer_length - 1]\n\n                            answer_end = answer_start + len(original_answer)\n                        else:\n                            original_answer = ''\n                            start_pos = 1\n                            end_pos = -1\n\n                        example = SquadExample(\n                            qid=qa['id'],\n                            context=doc_tokens,\n                            question=qa['question'],\n                            answer=original_answer,\n                            start=start_pos,\n                            end=end_pos,\n                            is_impossible=is_impossible)\n                        self.examples.append(example)\n            print('examples: {}'.format(len(self.examples)))\n\n            self.features = convert_examples_to_features(\n                examples=self.examples,\n                tokenizer=tokenizer,\n                max_seq_length=384,\n                doc_stride=128,\n                max_query_length=64,\n                is_training=True if not is_inference else False)\n            print('is_training: {}'.format(True if not is_inference else False))\n\n            # torch.save(self.examples, cached_examples_file)\n            # torch.save(self.features, cached_features_file)\n\n        '''\n        # Convert to Tensors and build dataset\n        all_input_ids = torch.tensor([f.input_ids for f in self.features], dtype=torch.long)\n        all_input_mask = torch.tensor([f.input_mask for f in self.features], dtype=torch.long)\n        all_segment_ids = torch.tensor([f.segment_ids for f in self.features], dtype=torch.long)\n        all_cls_index = torch.tensor([f.cls_index for f in self.features], dtype=torch.long)\n        all_p_mask = torch.tensor([f.p_mask for f in self.features], dtype=torch.float)\n        if is_train:\n            all_start_positions = torch.tensor([f.start_position for f in self.features], dtype=torch.long)\n            all_end_positions = torch.tensor([f.end_position for f in self.features], dtype=torch.long)\n            dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n                                    all_start_positions, all_end_positions,\n                                    all_cls_index, all_p_mask)\n        else:\n            all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n            dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_example_index, all_cls_index, all_p_mask)\n        return dataset\n        '''\n\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n        return self.features[idx]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-02T09:13:53.551391Z","iopub.execute_input":"2024-06-02T09:13:53.551756Z","iopub.status.idle":"2024-06-02T09:13:53.585501Z","shell.execute_reply.started":"2024-06-02T09:13:53.551726Z","shell.execute_reply":"2024-06-02T09:13:53.584786Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### 실습 4.3 - SQuAD DataLoader 생성 (from raw data to tokenized version)","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\n\nclass SquadDataLoader(DataLoader):\n    def __init__(self, dataset, batch_size, is_inference=False, shuffle=True):\n        '''\n        dataset: SquadDataset으로 정의한 데이터셋 객체\n        batch_size: 배치 사이즈\n        is_inference: SquadDataLoader를 인퍼런스 목적으로 사용할 경우 True, 그렇지 않으면 False\n        shuffle: 데이터의 순서를 섞을 경우 True, 그렇지 않으면 False\n        '''\n        self.is_inference = is_inference\n        super().__init__(dataset, collate_fn=self.squad_collate_fn, batch_size=batch_size, shuffle=shuffle)\n        \n    def squad_collate_fn(self, features):\n        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n        all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n        all_cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)\n        all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)\n\n        # return 6 tensors\n        if self.is_inference:\n            all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n            return all_input_ids, all_input_mask, all_segment_ids, all_cls_index, all_p_mask, all_example_index\n        # return 7 tensors\n        else:\n            all_start_positions = torch.tensor([f.start_position for f in features], dtype=torch.long)\n            all_end_positions = torch.tensor([f.end_position for f in features], dtype=torch.long)\n            return all_input_ids, all_input_mask, all_segment_ids, all_cls_index, all_p_mask, all_start_positions, all_end_positions","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:14:06.527600Z","iopub.execute_input":"2024-06-02T09:14:06.528225Z","iopub.status.idle":"2024-06-02T09:14:06.539235Z","shell.execute_reply.started":"2024-06-02T09:14:06.528194Z","shell.execute_reply":"2024-06-02T09:14:06.537947Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### 실습 4.4 Load Dataset","metadata":{}},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm, trange\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\npath = \"/kaggle/input/2024-1-nlp-4/\"\n\nfrom transformers import BertTokenizer\n\nprint(\"Tokenizer Loading\")\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\nprint(\"Dataset Loading\")\ntrain_dataset = SquadDataset(path, tokenizer, is_train=True) # 153,000\n\nprint(\"Data Loader\")\ntrain_dataloader = SquadDataLoader(train_dataset, batch_size=32, is_inference=False, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:14:13.992941Z","iopub.execute_input":"2024-06-02T09:14:13.993309Z","iopub.status.idle":"2024-06-02T09:38:30.316438Z","shell.execute_reply.started":"2024-06-02T09:14:13.993280Z","shell.execute_reply":"2024-06-02T09:38:30.315465Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Tokenizer Loading\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbef7e64e1f0466f8a4e5c29e95ebbec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5768b51dccd84dff9b2a01d6191970fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40b81f60fd51430abf0b5c8c5b87e51c"}},"metadata":{}},{"name":"stdout","text":"Dataset Loading\ncache file does not exist\nexamples: 130319\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 130319/130319 [24:00<00:00, 90.46it/s] ","output_type":"stream"},{"name":"stdout","text":"is_training: True\nData Loader\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 실습 4.5 - Load Pre-trained BERT\n### 과제 4.1 - BERT for Question Answering 모델 이해하고 설명하기 / Tokenizer 변경해보기\n\n#### BERT for Question Answering 참고\n#### https://huggingface.co/docs/transformers/v4.41.0/en/model_doc/bert#transformers.BertForQuestionAnswering\n\n#### BERT Tokenizer 참고\n#### https://huggingface.co/docs/transformers/v4.41.0/en/model_doc/bert#transformers.BertTokenizer","metadata":{}},{"cell_type":"code","source":"# pytoch model import from huggingface\nfrom transformers import BertTokenizer, BertForQuestionAnswering, AdamW\n\n# GPU 이용 방법 - Notebook Option - Session Options - ACCELRATOR 설정 (GPU P100)\n# .cuda() 옵션을 제거하면 cpu에서도 학습 가능\nmodel = BertForQuestionAnswering.from_pretrained('bert-base-uncased').cuda()\n\nmodel.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:11:01.582632Z","iopub.execute_input":"2024-06-02T09:11:01.582979Z","iopub.status.idle":"2024-06-02T09:11:20.341551Z","shell.execute_reply.started":"2024-06-02T09:11:01.582951Z","shell.execute_reply":"2024-06-02T09:11:20.340606Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a2c16363f88465ab27b3c9f6c8ceeab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16f4dc96cb88464b8f0ec6df766e60f2"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"BertForQuestionAnswering(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"### 실습 4.6 - Fine-tuning with your SQuAD Dataset","metadata":{}},{"cell_type":"markdown","source":"주석 달아","metadata":{}},{"cell_type":"code","source":"# train fucntion\ndef train(model, dataloader, optimizer):\n    tbar = tqdm(dataloader, desc='Training', leave=True)\n    \n    total_loss = 0.0\n    for i, batch in enumerate(tbar):\n        optimizer.zero_grad()\n        \n        # cls_index와 p_mask는 XLNet 모델에 사용되므로 BERT에서는 사용하지 않는다.\n        input_ids, input_mask, segment_ids, cls_index, p_mask, start_positions, end_positions = batch\n        \n        # to cuda (gpu 사용 시)\n        input_ids = input_ids.cuda()\n        input_mask = input_mask.cuda()\n        segment_ids = segment_ids.cuda()\n        start_positions = start_positions.cuda()\n        end_positions = end_positions.cuda()\n        \n        # train model\n        #out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        inputs = {\n            'input_ids': input_ids,\n            'token_type_ids': segment_ids,\n            'attention_mask': input_mask,\n        }\n        out = model(**inputs, start_positions=start_positions, end_positions=end_positions)\n        loss = out.loss\n\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.data.item()\n        tbar.set_description(\"Average Loss = {:.4f})\".format(total_loss/(i+1)))","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:38:42.961506Z","iopub.execute_input":"2024-06-02T09:38:42.962851Z","iopub.status.idle":"2024-06-02T09:38:42.971309Z","shell.execute_reply.started":"2024-06-02T09:38:42.962818Z","shell.execute_reply":"2024-06-02T09:38:42.970390Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\"\"\" 원래 코드\nTrain (Fine-tune) your BERT with SQuAD dataset\n\"\"\"\n\noptimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\nloss = nn.CrossEntropyLoss()\nn_epoch = 10\n\n# actual training\nfor i in range(n_epoch):\n    train(model, train_dataloader, optimizer)\n\n\n# save model\n# torch.save(model.state_dict(), 'squad_model.bin')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" 13시간 돌린 모델\nTrain (Fine-tune) your BERT with SQuAD dataset\n\"\"\"\n\nimport torch\nfrom transformers import BertTokenizer, BertForQuestionAnswering, AdamW\n\nmodel = BertForQuestionAnswering.from_pretrained('bert-base-uncased').cuda()\n\nmodel.load_state_dict(torch.load('/kaggle/input/squad-model-ep10/squad_model_ep10.bin'))","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:11:43.009814Z","iopub.execute_input":"2024-06-02T09:11:43.010475Z","iopub.status.idle":"2024-06-02T09:11:46.902457Z","shell.execute_reply.started":"2024-06-02T09:11:43.010445Z","shell.execute_reply":"2024-06-02T09:11:46.901566Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"### 과제 4.2 Inference 및 Evaluate\n\n- 파인튜닝을 마치고 dev-v2.json 파일을 불러와 Inference를 위한 코드를 실행한다.\n- 예측한 span과 정답 span을 비교해본다.\n- F1을 이용하여 dev-v2.json의 샘플 1000개를 대상으로 예측한 span과 정답 span을 평가하는 코드를 작성한다.\n\n아래 평가용 코드 참고\n\n- https://github.com/jinkilee/hello-transformer/blob/master/research/chapter4/squad/run_evaluate.py\n- https://github.com/jinkilee/hello-transformer/blob/master/research/chapter4/squad/evaluate.py","metadata":{}},{"cell_type":"code","source":"valid_dataset = SquadDataset(path, tokenizer, is_train=False) # 11,873\nvalid_dataloader = SquadDataLoader(valid_dataset, batch_size=32, is_inference=False, shuffle=True)\n\n# 학습된 모델이 예측한 결과와 주어진 validation 데이터셋과 비교해본다.\ndef inference(model, tokenizer):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    \n    input_text = \"입력 텍스트를 여기에 넣으세요\"\n    encoded_input = tokenizer(input_text, return_tensors='pt')\n    encoded_input = encoded_input.to(device)\n    \n    with torch.no_grad():\n        output = model(**encoded_input)\n    \n    predicted_label = torch.argmax(output.logits, dim=-1)\n    print(f\"예측 결과: {predicted_label}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fine-tuned된 데이터셋을 평가한다.\n\ndef evaluate(model, tokenizer):\n\"\"\"\nWrite your code here\n\"\"\"\n\n    \ndef main():\n    # 모델 정의\n    model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\", num_labels = 2).to(device)\n    #model.load_state_dict(torch.load('models/squad_model.bin'))\n    model.eval()\n\n    model.to(args.device)\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n    evaluate(model, tokenizer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_dataset = SquadDataset(path, tokenizer, is_train=False) # 11,873\nvalid_dataloader = SquadDataLoader(valid_dataset, batch_size=32, is_inference=False, shuffle=True)\n\n# 학습된 모델이 예측한 결과와 주어진 validation 데이터셋과 비교해본다.\ndef inference(model, tokenizer):\n    model.eval()\n    predictions = []\n\n    for batch in tqdm(valid_dataloader, desc=\"Inference\"):\n        with torch.no_grad():\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            \n            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n            start_scores, end_scores = outputs.start_logits, outputs.end_logits\n            \n            for i in range(input_ids.size(0)):\n                start_idx = torch.argmax(start_scores[i]).item()\n                end_idx = torch.argmax(end_scores[i]).item()\n                \n                answer_ids = input_ids[i][start_idx:end_idx+1]\n                answer = tokenizer.decode(answer_ids, skip_special_tokens=True)\n                predictions.append(answer)\n    \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:38:51.445802Z","iopub.execute_input":"2024-06-02T09:38:51.446424Z","iopub.status.idle":"2024-06-02T09:41:13.357101Z","shell.execute_reply.started":"2024-06-02T09:38:51.446391Z","shell.execute_reply":"2024-06-02T09:41:13.356066Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"cache file does not exist\nexamples: 11873\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11873/11873 [02:20<00:00, 84.22it/s] ","output_type":"stream"},{"name":"stdout","text":"is_training: True\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Fine-tuned된 데이터셋을 평가한다.\ndef evaluate(model, tokenizer):\n    predictions = inference(model, tokenizer)\n    f1_scores = []\n\n    for prediction, reference in zip(predictions, references):\n        f1 = compute_f1(prediction, reference)\n        f1_scores.append(f1)\n    \n    average_f1 = sum(f1_scores) / len(f1_scores)\n    print(f\"Average F1 Score: {average_f1:.4f}\")\n\ndef compute_f1(pred, truth):\n    pred_tokens = pred.split()\n    truth_tokens = truth.split()\n    \n    common = set(pred_tokens) & set(truth_tokens)\n    if len(common) == 0:\n        return 0.0\n    \n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(truth_tokens)\n    f1 = 2 * (precision * recall) / (precision + recall)\n    return f1\n\ndef main():\n    \n    # Load validation data\n    with open('/kaggle/input/2024-1-nlp-4/dev-v2.json', 'r') as f:\n        dev_data = json.load(f)\n\n    # references 추출\n    global references\n    references = []\n    for article in dev_data['data']:\n        for paragraph in article['paragraphs']:\n            for qa in paragraph['qas']:\n                if 'answers' in qa and len(qa['answers']) > 0:\n                    references.append(qa['answers'][0]['text'])\n                else:\n                    references.append('')\n\n    global valid_dataset\n    valid_dataset = SquadDataset('dev-v2.json', tokenizer, is_train=False) # 11,873\n    global valid_dataloader\n    valid_dataloader = SquadDataLoader(valid_dataset, batch_size=32, is_inference=False, shuffle=True)\n\n    evaluate(model, tokenizer)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:53:38.026091Z","iopub.execute_input":"2024-06-02T09:53:38.027026Z","iopub.status.idle":"2024-06-02T09:53:38.223500Z","shell.execute_reply.started":"2024-06-02T09:53:38.026993Z","shell.execute_reply":"2024-06-02T09:53:38.221999Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"cache file does not exist\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m     evaluate(model, tokenizer)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[21], line 44\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m                 references\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m valid_dataset\n\u001b[0;32m---> 44\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSquadDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdev-v2.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 11,873\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m valid_dataloader\n\u001b[1;32m     46\u001b[0m valid_dataloader \u001b[38;5;241m=\u001b[39m SquadDataLoader(valid_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, is_inference\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","Cell \u001b[0;32mIn[5], line 64\u001b[0m, in \u001b[0;36mSquadDataset.__init__\u001b[0;34m(self, path, tokenizer, is_train, is_inference)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache file does not exist\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[1;32m     65\u001b[0m         input_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(reader)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexamples \u001b[38;5;241m=\u001b[39m []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dev-v2.json/dev-v2.json'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'dev-v2.json/dev-v2.json'","output_type":"error"}]}]}